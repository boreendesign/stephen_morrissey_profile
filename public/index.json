[{"authors":["admin"],"categories":null,"content":"I am passionate about new and innovate technologies, user-centered design, and data-driven development. Since starting off as a front end developer in the early 2000\u0026rsquo;s to positions in Program and Product, I have continued to keep my engineering skills fresh through courses and weekend projects. In Program and Product positions I have found it important to be able to translate effectively between different discipline groups whether that be UX, Technology or C-Suite Stakeholders.\nKey accomplishments in my previous role at Sinclair have been 1) Managing from a program and then product perspective,100+ websites across the US such as komonews.com 2) Managing Integrations in the advertising space on Web, Mobile Apps and OTT (Connected TV\u0026rsquo;s such as Roku) with technologies such as Google Ad Manager, Rubicon Open Pre Bid, JWPlayer and SpotX\nDuring my time as a Technical Program/Project Manager, I oversaw all facets of the project lifecycle; from planning, risk assessment, resource management, product development, QA to delivery. I believe the keys to a successful project are: 1) building strong working relationships with coworkers and clients 2) staying on top of current technologies with doing some quick projects 3) the ability to look at problems/opportunities from a high level and work backward from a common goal.\nAs mentioned, in my free time, I love working on weekend projects (kind of like my sudoku) with various colleagues, often building them from scratch or using a CMS as the underlying framework. Having 7 years of experience working with custom CMS-driven websites using Drupal.\nRecently I have been focussing on increasing my knowledge in the AI/ML space and working through various courses and projects such as: 1) Google Cloud Platform Fundamentals 2) IBM Data Science Certifications I love a challenge, working with teams to structure a solution and then delivering that solution while monitoring impact.\n","date":1461110400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1555459200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://stephenmorrissey.me/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am passionate about new and innovate technologies, user-centered design, and data-driven development. Since starting off as a front end developer in the early 2000\u0026rsquo;s to positions in Program and Product, I have continued to keep my engineering skills fresh through courses and weekend projects. In Program and Product positions I have found it important to be able to translate effectively between different discipline groups whether that be UX, Technology or C-Suite Stakeholders.","tags":null,"title":"Stephen Morrissey","type":"authors"},{"authors":null,"categories":null,"content":" Steps  Create a Drupal 8 Site locally using composer Setup the Rest API Create and run the python script to create the content Import content from a CSV  GOTCHA\u0026rsquo;S PLEASE READ BELOW\n In newer versions of Drupal 8, the Rest UI doesnt seem to always work, I would recommend creating by manually as below. Also install Drush and use drush cr to clear the cache if you get any network errors If you run into issues on drupal, try using drush cr and POSTMAN is a great tool to test out the endpoints to make sure everything is setup correctly  1. Create a Drupal 8 site locally using composer One of the great reasons to use composer to create your Drupal 8 site is that you can export configurations and iport them on your live site. (This was such a pain in earlier drupal sites but now this is major pain has a great solution).\n Use your webserver of choice (on MAC I use MAMP free version and will reference this as we go through the tutorial) Create your database, on MAMP  Go to \u0026lsquo;Open WebStart page\u0026rsquo; Tools -\u0026gt; PHPMYADMIN Databases -\u0026gt; Create Database -\u0026gt; Enter the name of your Database (for this tutorial : drupal_8_python) You may also need to create a user for this database and assign if you have not already done so (for this tutorial : username:drupal_8_python, password:drupal_8_python_2020)  Add your new url to the hosts file, on MAC run sudo atom /etc/hosts and then enter 127.0.0.1 drupal_8_python.local Create your vhosts in the MAMP folder, if you have the ATOM editor installed atom /Applications/MAMP/conf/apache/extra/httpd-vhosts.conf  Add the following (see code snippet below - Apache Vhosts Code Snippet)  Install composer (lots of info online for this for different operating systems) Go to the folder on your local system where you want to install your new drupal 8 site composer create-project drupal-composer/drupal-project:8.x-dev INSERT_YOUR_NEW_SITE_HERE --no-interaction for this example we will use composer create-project drupal-composer/drupal-project:8.x-dev drupal_8_python --no-interaction Restart your webserver Then vist http://drupal_8_python.local and continue with the installation of your new site entering the database details from earlier You should then be able to login at http://drupal_8_python.local/user Thats the end of this part, you have now gotten Drupal 8 successfully installed with Composer, nice job and you now can do configuration exports when you want to go to production!  Apache Vhosts Code Snippet \u0026lt;VirtualHost *:80\u0026gt; DocumentRoot [INSERT LOCATION TO YOUR DRUPAL FOLDER]/drupal_8_python/web ServerName drupal_8_python.local \u0026lt;Directory [INSERT LOCATION TO YOUR DRUPAL FOLDER]/drupal_8_python/web\u0026gt; Options FollowSymLinks MultiViews Order deny,allow Allow from all \u0026lt;/Directory\u0026gt; \u0026lt;/VirtualHost\u0026gt;  2. Setup the Rest API  Enable the following modules by going to /admin/modules  HAL HTTP Basic Authentication RESTful Web Services Serialization  Now we are going to install the Rest UI module on the command line with composer require drupal/restui Install the module through EXTEND on your drupal site and go to /admin/config/services/rest/resource/entity%3Anode/edit Configuration Screen:  Now create a sample piece of content so we have something to search for to test our rest API We now should also create a user and role to access our API, I created a role call api_access and a user called \u0026lsquo;api_admin\u0026rsquo;, make sure that the api_admin user has the api_access role Next go to the permissions for this user and make sure they have access to create, view and delete the content you want to be able to create later using Python or another language You can now test this out if you like with the POSTMAN tool that you can download (Go to Screenshots below to see what to put in each tab)  To get the value for X-CSRF-Token, go to [YOUR DOMAIN]/session/token and paste in the value, should be something like yvxnn50NYMfMF2IEIn6s7vMvW3CQQVLxcb6njPU3InA  IF THE ABOVE FAILS :Go to the command line and type drush cex Now from the cmd line \u0026lsquo;atom config/sync/rest.resource.entity.node.yml\u0026rsquo; Check the contents of the file with the code block below - REST IMPORT THROUGH CONFIG You are now ready for the next section which is importing nodes through python  REST IMPORT THROUGH CONFIG langcode: en status: true dependencies: module: - basic_auth - hal - node - serialization id: entity.node plugin_id: 'entity:node' granularity: resource configuration: methods: - GET - POST - PATCH - DELETE formats: - hal_json - json authentication: - basic_auth  ##3 SETUP POSTMAN Url and Params: Authorization: Headers: Body: 3. Create and run the python script to create the content Below is the code that you can find in the attached Jupyter notebook or run straight as a python file. This was converted from the POSTMAN commands from above.\n# NOTE IF THERE ARE ERRORS - TRYING DRUSH CR import requests base_url = 'http://drupal_8_python.local' #Get CSRF token token = str(requests.get(base_url + '/session/token').text) drupal_endpoint = base_url + '/node?_format=hal_json' #Set all required headers drupal_headers = {'Content-Type':'application/hal+json', 'X-CSRF-Token':token } #Include all fields required by the content type, using basic examples but from this you can add any fields including custom ones drupal_payload = '''{ \u0026quot;_links\u0026quot;: { \u0026quot;type\u0026quot;: { \u0026quot;href\u0026quot;: \u0026quot;'''+base_url+'''/rest/type/node/page\u0026quot; } }, \u0026quot;title\u0026quot;:[{\u0026quot;value\u0026quot;:\u0026quot;Sample Drupal Node Page\u0026quot;}], \u0026quot;body\u0026quot;:[{\u0026quot;value\u0026quot;:\u0026quot;Body with some sample content\u0026quot;}] }'''  #Post the new node (a Contact) to the endpoint. r = requests.post(drupal_endpoint, data=drupal_payload, headers=drupal_headers, auth=('api_user','PASSWORD')) #Check was a success if r.status_code == 201: print(\u0026quot;Success\u0026quot;) else: print(\u0026quot;Fail\u0026quot;) print(r.status_code) print(r.text)  Success  4. Import content from a CSV I will be updating the python script to import from a CSV but please check out another article I wrote from scraping, it should be pretty easy in the meantime to use this to create a CSV and then import with the above script.SCraping data to a csv file with Python\n*I wrote all of this as I was going through the steps from scratch, except the initial installs of composer, jupyter notebooks, drush and postman, please do let me know in the comments if I got anything wrong, missed anything or need to go into detail or provide external links to, thanks and I hope it helps a few of you, this all took me quite some time to work through the kinks, hopefully it will save you the time. *\nGet the code\n","date":1573171200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573171200,"objectID":"8e8c6c612dc6fa3af2b8ed6b83947fb9","permalink":"https://stephenmorrissey.me/project/import_to_drupal/","publishdate":"2019-11-08T00:00:00Z","relpermalink":"/project/import_to_drupal/","section":"project","summary":"An example of creating a Drupal site from imported nodes using Python from a CSV","tags":["Python","Drupal"],"title":"Use python to create nodes in Drupal from a CSV file","type":"project"},{"authors":null,"categories":null,"content":" Scrap Content and Build a Hugo Site This is the first part of a project I worked on that using Python, Beautiful Soup, Hugo, Netlify, Zapier and Datatables to create a site that scraped pricing data every 24 hours and rendered it on a static site.\n PART 1 : Scrape the content to a JSON and CSV file (This Article) PART 2 : Build the Hugo site and pull in the JSON file for rendering (coming soon) PART 3 : Deploy to Netlify and Schedule to run every 24 hours (coming soon)  Requirements  You need to have python 3 and pip installed You need to install all the imported modules below using pip eg pip install requests I recommend installing Jupyter notebooks, its an easy way to test your code in python and seeing the results after each code block  Steps  Setup and Helper Functions Import the Site Data from a CSV For Each Site for teaser content, scrape the data (Title, Price and Url to the more information page) Combine all the site data into a data frame Export this data to a json and csv file for use on the hugo site and other projects (Drupal Import ) Build the hugo site  Setup and Helper Functions Import the neccesary python modules import requests from bs4 import BeautifulSoup import pandas as pd from datetime import datetime import re import hashlib import subprocess  Variable Setup timestamp_full = datetime.today().strftime('%Y-%m-%d-%H:%M:') timestamp_day = datetime.today().strftime('%Y-%m-%d') # Site Import Data site_data_file = 'importfiles/sitedata.csv' site_data_df = pd.DataFrame(columns=[\u0026quot;company\u0026quot;,\u0026quot;site\u0026quot;, \u0026quot;collection\u0026quot;]) # Site Export Data scraped_data_file = 'exportfiles/scraped_sites'  Standard Helper Functions # Log processing commands to see where a site may have failed def log_processing(url): print(' ---\u0026gt; processing ' + url) # Pull out everything but numbers and letters from the imported string def returnNumbersAndLettersOnly(oldString): newString = ''.join(e for e in oldString if e.isalnum()) return newString # Pull out everything but numbers from the imported string def returnNumbersOnly(oldString): newString = ''.join(e for e in oldString if e.isdigit()) return newString # Set default headers for call # Testing out calls def getHeadersObject(url): headers = { 'User-Agent': \u0026quot;PostmanRuntime/7.18.0\u0026quot;, 'Accept': \u0026quot;*/*\u0026quot;, 'Cache-Control': \u0026quot;no-cache\u0026quot;, 'Accept-Encoding': \u0026quot;gzip, deflate\u0026quot;, 'Referer': url, 'Connection': \u0026quot;keep-alive\u0026quot;, 'cache-control': \u0026quot;no-cache\u0026quot; } return headers # Get the index of an item and handle the exception where the index does not exist and set to empty def pop(item,index): try: return item[index] except IndexError: return 'null' return breakout  Custom Functions # Sample Scraping Function def scrapeTeaserDataFromCollection_type1(url,site): site_scraped_data_df_temp = pd.DataFrame(columns=[\u0026quot;site\u0026quot;,\u0026quot;title\u0026quot;, \u0026quot;url\u0026quot;,\u0026quot;price\u0026quot;]) soup = BeautifulSoup(requests.get(url).text, 'html.parser') resultsRow = soup.find_all('article', {'class': 'box-info'}) for resultRow in resultsRow: resultRowBreakdown = resultRow.find('p', {'class': 'text-info'}).text.split('from') try: site_scraped_data_df_temp = site_scraped_data_df_temp.append( { 'site':site, 'title':pop(resultRowBreakdown,0), 'price':returnNumbersOnly(pop(resultRowBreakdown,1)), 'url':resultRow.find('a').get('href') } , ignore_index=True) except IndexError as e: gotdata = '' return site_scraped_data_df_temp # Sample Scraping Function def scrapeTeaserDataFromCollection_type2(url,site): site_scraped_data_df_temp = pd.DataFrame(columns=[\u0026quot;site\u0026quot;,\u0026quot;title\u0026quot;, \u0026quot;url\u0026quot;,\u0026quot;price\u0026quot;]) soup = BeautifulSoup(requests.get(url).text, 'html.parser') resultsRow = soup.find('div', {'class': 'ppb_tour_classic'}).find_all('div', {'class': 'element'}) for resultRow in resultsRow: try: site_scraped_data_df_temp = site_scraped_data_df_temp.append( { 'site':site, 'title':resultRow.find('h4').text, #\u0026lt;h2\u0026gt;\u0026lt;a -.find('a'), 'price':returnNumbersOnly(resultRow.find('div', {'class': 'tour_price'}).text.strip('$')), 'url':resultRow.find('a',{'class': 'tour_link'}).get('href') } , ignore_index=True) except IndexError as e: gotdata = '' return site_scraped_data_df_temp  Import the site data # Import and Print out the Sites you want to Scrape site_data_df = pd.read_csv(site_data_file) site_data_df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    company site collection scrape_type     0 Stephen Morrissey 1 https://stephenmorrissey.me https://stephenmorrissey.me/post/type1-content/ type1   1 Stephen Morrissey 2 https://stephenmorrissey.me https://stephenmorrissey.me/post/type1-content/ type1     Scrape Pages site_scraped_data_df = pd.DataFrame(columns=[\u0026quot;site\u0026quot;,\u0026quot;title\u0026quot;, \u0026quot;url\u0026quot;,\u0026quot;price\u0026quot;]) #Go Through All Sites and Scrape the Appropriate Data for index, row in site_data_df.iterrows(): log_processing(row['company']) if row['scrape_type'] == 'type1': site_scraped_data_df = site_scraped_data_df.append(scrapeTeaserDataFromCollection_type1(row['collection'],row['company']), ignore_index=True) elif row['scrape_type'] == 'type2': site_scraped_data_df = site_scraped_data_df.append(scrapeTeaserDataFromCollection_type2(row['collection'],row['company']), ignore_index=True) else: print(\u0026quot;NOT SCRAPE TYPE FOUND\u0026quot;)   ---\u0026gt; processing Stephen Morrissey 1 ---\u0026gt; processing Stephen Morrissey 2  Output results to CSV \u0026amp; JSON files #Output to CSV site_scraped_data_df.to_csv(scraped_data_file+'.csv') #Output to JSON (Orient will allow you to input this into Datatables.net structure for display on a site) site_scraped_data_df.to_json(scraped_data_file+'.json',orient='records') print(site_scraped_data_df)   site title url price 0 Stephen Morrissey 1 Sample Page Content /testurl 3200 1 Stephen Morrissey 1 Sample Page Content 2 /testurl 900 2 Stephen Morrissey 1 Sample Page Content 3 /testurl 1300 3 Stephen Morrissey 2 Sample Page Content /testurl 3200 4 Stephen Morrissey 2 Sample Page Content 2 /testurl 900 5 Stephen Morrissey 2 Sample Page Content 3 /testurl 1300  Get the code\nPART 2 - Building the Hugo Site to display the information on Netlify (Coming Soon)\n","date":1572480000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572480000,"objectID":"d9603b50b7a349d4e0a08bceddbd012d","permalink":"https://stephenmorrissey.me/project/python-scraping/","publishdate":"2019-10-31T00:00:00Z","relpermalink":"/project/python-scraping/","section":"project","summary":"An example of scraping sites using python and creating a json file for use in Hugo","tags":["Python","Static Site","Hugo","Beautiful Soup"],"title":"Scrap Content to build a Hugo Site - Part 1","type":"project"},{"authors":null,"categories":null,"content":"Samples for Testing Type 1   Sample Page Article Sample Page Content from$3200     Sample Page Article 2 Sample Page Content 2\nfrom$900     Sample Page Article 3 Sample Page Content 3\nfrom$1300     ","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"61c8d8b0350cb0c32bdb3279aa69e18e","permalink":"https://stephenmorrissey.me/post/type1-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/type1-content/","section":"post","summary":"Samples for Testing Type 1   Sample Page Article Sample Page Content from$3200     Sample Page Article 2 Sample Page Content 2\nfrom$900     Sample Page Article 3 Sample Page Content 3\nfrom$1300     ","tags":null,"title":"Type 1 Content","type":"post"},{"authors":["Stephen Morrissey"],"categories":["Demo"],"content":"A page for testing this\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"45a00e97fdbae12a005c08ed7a7d587b","permalink":"https://stephenmorrissey.me/post/sample-page-to-scrape/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/sample-page-to-scrape/","section":"post","summary":"Sample Page to Scrape","tags":["Academic"],"title":"Sample Page to Scrape","type":"post"}]