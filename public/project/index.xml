<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Stephen Morrissey</title>
    <link>/project/</link>
      <atom:link href="/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 31 Oct 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Projects</title>
      <link>/project/</link>
    </image>
    
    <item>
      <title>Scrap Content and Build a Hugo Site</title>
      <link>/project/internal-project/</link>
      <pubDate>Thu, 31 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/project/internal-project/</guid>
      <description>

&lt;h1 id=&#34;scrap-content-and-build-a-hugo-site&#34;&gt;Scrap Content and Build a Hugo Site&lt;/h1&gt;

&lt;h3 id=&#34;steps&#34;&gt;Steps&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Setup and Helper Functions&lt;/li&gt;
&lt;li&gt;Import the Site Data from a CSV&lt;/li&gt;
&lt;li&gt;For Each Site for teaser content, scrape the data (Title, Price and Url to the more information page)&lt;/li&gt;
&lt;li&gt;Combine all the site data into a data frame&lt;/li&gt;
&lt;li&gt;Export this data to a json and csv file for use on the hugo site and other projects (Drupal Import )&lt;/li&gt;
&lt;li&gt;Build the hugo site&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;setup-and-helper-functions&#34;&gt;Setup and Helper Functions&lt;/h2&gt;

&lt;h3 id=&#34;import-the-neccesary-python-modules&#34;&gt;Import the neccesary python modules&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import re
import hashlib
import subprocess
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;variable-setup&#34;&gt;Variable Setup&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;timestamp_full = datetime.today().strftime(&#39;%Y-%m-%d-%H:%M:&#39;)
timestamp_day  = datetime.today().strftime(&#39;%Y-%m-%d&#39;)

# Site Import Data
site_data_file = &#39;importfiles/sitedata.csv&#39;
site_data_df = pd.DataFrame(columns=[&amp;quot;company&amp;quot;,&amp;quot;site&amp;quot;, &amp;quot;collection&amp;quot;])

# Site Export Data
scraped_data_file = &#39;exportfiles/scraped_sites&#39;

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;standard-helper-functions&#34;&gt;Standard Helper Functions&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Log processing commands to see where a site may have failed
def log_processing(url):
    print(&#39;  ---&amp;gt; processing &#39; + url)

# Pull out everything but numbers and letters from the imported string
def returnNumbersAndLettersOnly(oldString):
    newString = &#39;&#39;.join(e for e in oldString if e.isalnum())
    return newString

# Pull out everything but numbers from the imported string
def returnNumbersOnly(oldString):
    newString = &#39;&#39;.join(e for e in oldString if e.isdigit())
    return newString

# Set default headers for call
# Testing out calls
def getHeadersObject(url):
    headers = {
        &#39;User-Agent&#39;: &amp;quot;PostmanRuntime/7.18.0&amp;quot;,
        &#39;Accept&#39;: &amp;quot;*/*&amp;quot;,
        &#39;Cache-Control&#39;: &amp;quot;no-cache&amp;quot;,
        &#39;Accept-Encoding&#39;: &amp;quot;gzip, deflate&amp;quot;,
        &#39;Referer&#39;: url,
        &#39;Connection&#39;: &amp;quot;keep-alive&amp;quot;,
        &#39;cache-control&#39;: &amp;quot;no-cache&amp;quot;
    }
    return headers

# Get the index of an item and handle the exception where the index does not exist and set to empty
def pop(item,index):

    try:
        return item[index]
    except IndexError:
        return &#39;null&#39;

    return breakout
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;custom-functions&#34;&gt;Custom Functions&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def scrapeTeaserDataFromCollection_type1(url,site):

    site_scraped_data_df_temp = pd.DataFrame(columns=[&amp;quot;site&amp;quot;,&amp;quot;title&amp;quot;, &amp;quot;url&amp;quot;,&amp;quot;price&amp;quot;])
    soup = BeautifulSoup(requests.get(url).text, &#39;html.parser&#39;)
    resultsRow = soup.find_all(&#39;article&#39;, {&#39;class&#39;: &#39;box-info&#39;})

    for resultRow in resultsRow:
        resultRowBreakdown = resultRow.find(&#39;p&#39;, {&#39;class&#39;: &#39;text-info&#39;}).text.split(&#39;\n&#39;)
        try:
            site_scraped_data_df_temp = site_scraped_data_df_temp.append(
                {
                    &#39;site&#39;:site,
                    &#39;title&#39;:pop(resultRowBreakdown,0),
                    &#39;price&#39;:returnNumbersOnly(pop(resultRowBreakdown,2)),
                    &#39;url&#39;:resultRow.find(&#39;a&#39;).get(&#39;href&#39;)
                }
            , ignore_index=True)
        except IndexError as e:
            gotdata = &#39;&#39;

    return site_scraped_data_df_temp

def scrapeTeaserDataFromCollection_type2(url,site):

    site_scraped_data_df_temp = pd.DataFrame(columns=[&amp;quot;site&amp;quot;,&amp;quot;title&amp;quot;, &amp;quot;url&amp;quot;,&amp;quot;price&amp;quot;])
    soup = BeautifulSoup(requests.get(url).text, &#39;html.parser&#39;)
    resultsRow = soup.find(&#39;div&#39;, {&#39;class&#39;: &#39;ppb_tour_classic&#39;}).find_all(&#39;div&#39;, {&#39;class&#39;: &#39;element&#39;})

    for resultRow in resultsRow:
        try:
            site_scraped_data_df_temp = site_scraped_data_df_temp.append(
                {
                    &#39;site&#39;:site,
                    &#39;title&#39;:resultRow.find(&#39;h4&#39;).text, #&amp;lt;h2&amp;gt;&amp;lt;a -.find(&#39;a&#39;),
                    &#39;price&#39;:returnNumbersOnly(resultRow.find(&#39;div&#39;, {&#39;class&#39;: &#39;tour_price&#39;}).text.strip(&#39;$&#39;)),
                    &#39;url&#39;:resultRow.find(&#39;a&#39;,{&#39;class&#39;: &#39;tour_link&#39;}).get(&#39;href&#39;)
                }
            , ignore_index=True)
        except IndexError as e:
            gotdata = &#39;&#39;

    return site_scraped_data_df_temp
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;import-the-site-data&#34;&gt;Import the site data&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import and Print out the Sites you want to Scrape
site_data_df = pd.read_csv(site_data_file)
site_data_df
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;company&lt;/th&gt;
      &lt;th&gt;site&lt;/th&gt;
      &lt;th&gt;collection&lt;/th&gt;
      &lt;th&gt;scrape_type&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Example Site 1&lt;/td&gt;
      &lt;td&gt;https://www.examplesite.com&lt;/td&gt;
      &lt;td&gt;https://www.example.com/bloglist/&lt;/td&gt;
      &lt;td&gt;type1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Example Site 2&lt;/td&gt;
      &lt;td&gt;https://www.examplesite.com&lt;/td&gt;
      &lt;td&gt;https://www.example.com/bloglist/&lt;/td&gt;
      &lt;td&gt;type2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h2 id=&#34;scrape-pages&#34;&gt;Scrape Pages&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;site_scraped_data_df = pd.DataFrame(columns=[&amp;quot;site&amp;quot;,&amp;quot;title&amp;quot;, &amp;quot;url&amp;quot;,&amp;quot;price&amp;quot;])

#Go Through All Sites and Scrape the Appropriate Data

for index, row in site_data_df.iterrows():
    log_processing(row[&#39;company&#39;])
    if row[&#39;scrape_type&#39;] == &#39;type1&#39;:
        site_scraped_data_df = site_scraped_data_df.append(scrapeTeaserDataFromCollection(row[&#39;collection&#39;],row[&#39;company&#39;]), ignore_index=True)
    elif row[&#39;scrape_type&#39;] == &#39;type2&#39;:
        site_scraped_data_df = site_scraped_data_df.append(scrapeTeaserDataFromCollection_type2(row[&#39;collection&#39;],row[&#39;company&#39;]), ignore_index=True)
    else:
        print(&amp;quot;NOT SCRAPE TYPE FOUND&amp;quot;)



&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;  ---&amp;gt; processing Example Site 1
NOT SCRAPE TYPE FOUND
  ---&amp;gt; processing Example Site 2
NOT SCRAPE TYPE FOUND
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;output-results-to-csv-json-files&#34;&gt;Output results to CSV &amp;amp; JSON files&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Output to CSV
site_scraped_data_df.to_csv(scraped_data_file+&#39;.csv&#39;)
#Output to JSON (Orient will allow you to input this into Datatables.net structure for display on a site)
site_scraped_data_df.to_json(scraped_data_file+&#39;.json&#39;,orient=&#39;records&#39;)
print(site_scraped_data_df)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Empty DataFrame
Columns: [site, title, url, price]
Index: []
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/boreendesign/public_projects/blob/master/Scrape%20Content%20and%20Build%20Hugo%20Site.ipynb&#34; target=&#34;_blank&#34;&gt;Get the code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;PART 2 - Building the Hugo Site to display the information on Netlify (Coming Soon)&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
